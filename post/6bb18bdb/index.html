<!DOCTYPE html><html lang="zh-CN" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Automatic Differentiation | DEAL-II-Fluid</title><meta name="robots" content="noindex"><meta name="description" content="微分的应用价值 最优化问题   min_{x\in R^n} f(x)using the gradient descent method:  x_{n+1} &#x3D; x_n - \alpha_n \nabla f(x_n) 敏感性分析  f(x + \nabla x) \approx f&#39;(x) \Delta x 机器学习 利用自动微分（back-propagation）来训练神经网络  求解非线性方"><meta name="keywords" content="SciML"><meta name="author" content="Jiaqi"><meta name="copyright" content="Jiaqi"><meta name="format-detection" content="telephone=no"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="http://deal-ii.com/post/6bb18bdb/"><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//fonts.googleapis.com" crossorigin="crossorigin"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="preconnect" href="//zz.bdstatic.com"/><meta property="og:type" content="article"><meta property="og:title" content="Automatic Differentiation"><meta property="og:url" content="http://deal-ii.com/post/6bb18bdb/"><meta property="og:site_name" content="DEAL-II-Fluid"><meta property="og:description" content="微分的应用价值 最优化问题   min_{x\in R^n} f(x)using the gradient descent method:  x_{n+1} &#x3D; x_n - \alpha_n \nabla f(x_n) 敏感性分析  f(x + \nabla x) \approx f&#39;(x) \Delta x 机器学习 利用自动微分（back-propagation）来训练神经网络  求解非线性方"><meta property="og:image" content="https://i.loli.net/2021/01/01/XuCqjioRr6mxPQn.png"><meta property="article:published_time" content="2021-01-01T02:58:33.000Z"><meta property="article:modified_time" content="2021-01-29T07:25:51.368Z"><meta name="twitter:card" content="summary"><script>var activateDarkMode = function () {
  document.documentElement.setAttribute('data-theme', 'dark')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#000')
  }
}
var activateLightMode = function () {
  document.documentElement.setAttribute('data-theme', 'light')
  if (document.querySelector('meta[name="theme-color"]') !== null) {
    document.querySelector('meta[name="theme-color"]').setAttribute('content', '#fff')
  }
}

var getCookies = function (name) {
  const value = `; ${document.cookie}`
  const parts = value.split(`; ${name}=`)
  if (parts.length === 2) return parts.pop().split(';').shift()
}

var autoChangeMode = 'false'
var t = getCookies('theme')
if (autoChangeMode === '1') {
  var isDarkMode = window.matchMedia('(prefers-color-scheme: dark)').matches
  var isLightMode = window.matchMedia('(prefers-color-scheme: light)').matches
  var isNotSpecified = window.matchMedia('(prefers-color-scheme: no-preference)').matches
  var hasNoSupport = !isDarkMode && !isLightMode && !isNotSpecified

  if (t === undefined) {
    if (isLightMode) activateLightMode()
    else if (isDarkMode) activateDarkMode()
    else if (isNotSpecified || hasNoSupport) {
      console.log('You specified no preference for a color scheme or your browser does not support it. I Schedule dark mode during night time.')
      var now = new Date()
      var hour = now.getHours()
      var isNight = hour <= 6 || hour >= 18
      isNight ? activateDarkMode() : activateLightMode()
    }
    window.matchMedia('(prefers-color-scheme: dark)').addListener(function (e) {
      if (Cookies.get('theme') === undefined) {
        e.matches ? activateDarkMode() : activateLightMode()
      }
    })
  } else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else if (autoChangeMode === '2') {
  now = new Date()
  hour = now.getHours()
  isNight = hour <= 6 || hour >= 18
  if (t === undefined) isNight ? activateDarkMode() : activateLightMode()
  else if (t === 'light') activateLightMode()
  else activateDarkMode()
} else {
  if (t === 'dark') activateDarkMode()
  else if (t === 'light') activateLightMode()
}</script><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Titillium+Web&amp;display=swap"><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: {"defaultEncoding":2,"translateDelay":0,"msgToTraditionalChinese":"繁","msgToSimplifiedChinese":"簡"},
  noticeOutdate: {"limitDay":500,"position":"top","messagePrev":"It has been","messageNext":"days since the last update, the content of the article may be outdated."},
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  bookmark: {
    message_prev: '按',
    message_next: '键将本页加入书签'
  },
  runtime_unit: '天',
  runtime: false,
  copyright: undefined,
  ClickShowText: undefined,
  medium_zoom: true,
  fancybox: true,
  Snackbar: {"bookmark":{"message_prev":"按","message_next":"键将本页加入书签"},"chs_to_cht":"你已切换为繁体","cht_to_chs":"你已切换为简体","day_to_night":"你已切换为深色模式","night_to_day":"你已切换为浅色模式","bgLight":"#49b1f5","bgDark":"#121212","position":"bottom-left"},
  justifiedGallery: {
    js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
    css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
  },
  baiduPush: true,
  highlightCopy: true,
  highlightLang: true,
  isPhotoFigcaption: false,
  islazyload: true,
  isanchor: false    
}</script><script>var GLOBAL_CONFIG_SITE = { 
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isSidebar: true,
  postUpdate: '2021-01-29 15:25:51'
}</script><noscript><style>
#nav {
  opacity: 1
}
.justified-gallery img{
  opacity: 1
}
</style></noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@master/Hexo/css/flink.min.css"><link rel="stylesheet" href="/gitcalendar/css/gitcalendar.css"/><meta name="generator" content="Hexo 5.3.0"></head><body><div id="mobile-sidebar"><div id="menu_mask"></div><div id="mobile-sidebar-menus"><div class="mobile_author_icon"><img class="avatar-img" src="https://cdn.jsdelivr.net/gh/Jiaqi-knight/imgBase@main/me.webp" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="mobile_post_data"><div class="mobile_data_item is-center"><div class="mobile_data_link"><a href="/archives/"><div class="headline">文章</div><div class="length_num">25</div></a></div></div><div class="mobile_data_item is-center">      <div class="mobile_data_link"><a href="/tags/"><div class="headline">标签</div><div class="length_num">33</div></a></div></div><div class="mobile_data_item is-center">     <div class="mobile_data_link"><a href="/categories/"><div class="headline">分类</div><div class="length_num">14</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 索引</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 工具</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/x-dealii-9.3"><i class="fa-fw fa-book"></i><span> deal.ii Library</span></a></li><li><a class="site-page" href="/x-swirlflow/"><i class="fa-fw fa fa-book"></i><span> swirlflow</span></a></li><li><a class="site-page" href="/x-codepen/"><i class="fa-fw fa fa-magic"></i><span> codepen</span></a></li><li><a class="site-page" href="/x-gallery/"><i class="fa-fw fa fa-beer"></i><span> gallery</span></a></li><li><a class="site-page" href="/x-shadertoy/"><i class="fa-fw fa fa-star"></i><span> shadertoy</span></a></li><li><a class="site-page" href="/x-webgl1/"><i class="fa-fw fa fa-camera-retro"></i><span> webgl1</span></a></li><li><a class="site-page" href="/x-webgl2/"><i class="fa-fw fa fa-camera-retro"></i><span> webgl2</span></a></li><li><a class="site-page" href="/x-markdown/"><i class="fa-fw fa fa-tree"></i><span> Vditor</span></a></li><li><a class="site-page" href="/x-DNN/"><i class="fa-fw fa fa-heartbeat"></i><span> DNN</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div></div></div><i class="fas fa-arrow-right on" id="toggle-sidebar"></i><div id="sidebar"><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar">     </div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BE%AE%E5%88%86%E7%9A%84%E5%BA%94%E7%94%A8%E4%BB%B7%E5%80%BC"><span class="toc-number">1.</span> <span class="toc-text">微分的应用价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%87%AA%E5%8A%A8%E5%BE%AE%E5%88%86"><span class="toc-number">2.</span> <span class="toc-text">自动微分</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Dual-Number"><span class="toc-number">3.</span> <span class="toc-text">Dual Number</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Tracing"><span class="toc-number">4.</span> <span class="toc-text">Tracing</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8"><span class="toc-number">5.</span> <span class="toc-text">实际应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Automatic-Differentiation-A-Brief-Intro"><span class="toc-number">6.</span> <span class="toc-text">Automatic Differentiation: A Brief Intro</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Comput-Graph"><span class="toc-number">6.1.</span> <span class="toc-text">Comput-Graph</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dynamic-Comput-Graphs-VS-Static-Comput-Graphs"><span class="toc-number">6.2.</span> <span class="toc-text">Dynamic Comput Graphs VS Static Comput Graphs</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Define-the-Nodes-in-the-Computational-Graph"><span class="toc-number">7.</span> <span class="toc-text">Define the Nodes in the Computational Graph</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Leaf-Nodes-%E5%8F%AF%E8%83%BD%E6%98%AF%E4%B8%AD%E9%97%B4%E8%8A%82%E7%82%B9"><span class="toc-number">7.1.</span> <span class="toc-text">Leaf Nodes 可能是中间节点</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Other-Nodes"><span class="toc-number">7.2.</span> <span class="toc-text">Other Nodes</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Evaluations"><span class="toc-number">8.</span> <span class="toc-text">Evaluations</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Forward-Evaluation"><span class="toc-number">8.1.</span> <span class="toc-text">Forward Evaluation</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Backward-Evaluation"><span class="toc-number">8.2.</span> <span class="toc-text">Backward Evaluation</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Broadcast"><span class="toc-number">9.</span> <span class="toc-text">Broadcast</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Add-more-operators-for-FREE"><span class="toc-number">10.</span> <span class="toc-text">Add more operators for FREE!</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Polish"><span class="toc-number">11.</span> <span class="toc-text">Polish</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Benchmark"><span class="toc-number">12.</span> <span class="toc-text">Benchmark</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Acknowledgement"><span class="toc-number">13.</span> <span class="toc-text">Acknowledgement</span></a></li></ol></div></div></div><div id="body-wrap"><div id="web_bg" data-type="photo"></div><header class="post-bg" id="page-header" style="background-image: url(https://i.loli.net/2021/01/01/XuCqjioRr6mxPQn.png)"><nav id="nav"><span class="pull-left" id="blog_name"><a class="blog_title" id="site-name" href="/">DEAL-II-Fluid</a></span><span class="pull-right menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fa fa-home"></i><span> 首页</span></a></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 索引</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/archives/"><i class="fa-fw fa fa-archive"></i><span> 时间轴</span></a></li><li><a class="site-page" href="/tags/"><i class="fa-fw fa fa-tags"></i><span> 标签</span></a></li><li><a class="site-page" href="/categories/"><i class="fa-fw fa fa-folder-open"></i><span> 分类</span></a></li></ul></div><div class="menus_item"><a class="site-page"><i class="fa-fw fas fa-list"></i><span> 工具</span><i class="fas fa-chevron-down menus-expand"></i></a><ul class="menus_item_child"><li><a class="site-page" href="/x-dealii-9.3"><i class="fa-fw fa-book"></i><span> deal.ii Library</span></a></li><li><a class="site-page" href="/x-swirlflow/"><i class="fa-fw fa fa-book"></i><span> swirlflow</span></a></li><li><a class="site-page" href="/x-codepen/"><i class="fa-fw fa fa-magic"></i><span> codepen</span></a></li><li><a class="site-page" href="/x-gallery/"><i class="fa-fw fa fa-beer"></i><span> gallery</span></a></li><li><a class="site-page" href="/x-shadertoy/"><i class="fa-fw fa fa-star"></i><span> shadertoy</span></a></li><li><a class="site-page" href="/x-webgl1/"><i class="fa-fw fa fa-camera-retro"></i><span> webgl1</span></a></li><li><a class="site-page" href="/x-webgl2/"><i class="fa-fw fa fa-camera-retro"></i><span> webgl2</span></a></li><li><a class="site-page" href="/x-markdown/"><i class="fa-fw fa fa-tree"></i><span> Vditor</span></a></li><li><a class="site-page" href="/x-DNN/"><i class="fa-fw fa fa-heartbeat"></i><span> DNN</span></a></li></ul></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fa fa-link"></i><span> 友链</span></a></div><div class="menus_item"><a class="site-page" href="/music/"><i class="fa-fw fa fa-music"></i><span> 音乐</span></a></div><div class="menus_item"><a class="site-page" href="/about/"><i class="fa-fw fa fa-heart"></i><span> 关于</span></a></div></div><span class="toggle-menu close"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></span></span></nav><div id="post-info"><div id="post-title"><div class="posttitle">Automatic Differentiation</div></div><div id="post-meta"><div class="meta-firstline"><time class="post-meta__date"><span class="post-meta__date-created" title="发表于 2021-01-01 10:58:33"><i class="far fa-calendar-alt fa-fw"></i> 发表于 2021-01-01</span><span class="post-meta__separator">|</span><span class="post-meta__date-updated" title="更新于 2021-01-29 15:25:51"><i class="fas fa-history fa-fw"></i> 更新于 2021-01-29</span></time><span class="post-meta__categories"><span class="post-meta__separator">|</span><i class="fas fa-inbox fa-fw post-meta__icon"></i><a class="post-meta__categories" href="/categories/code/">code</a></span></div><div class="meta-secondline"> </div><div class="meta-thirdline"><span class="post-meta-pv-cv"><i class="far fa-eye fa-fw post-meta__icon"></i><span>阅读量:</span><span id="busuanzi_value_page_pv"></span></span></div></div></div></header><main class="layout_post" id="content-inner"><article id="post"><div class="post-content" id="article-container"><h2 id="微分的应用价值"><a href="#微分的应用价值" class="headerlink" title="微分的应用价值"></a>微分的应用价值</h2><ul>
<li><p>最优化问题 </p>
<script type="math/tex; mode=display">
min_{x\in R^n} f(x)</script><p>using the gradient descent method:</p>
<script type="math/tex; mode=display">
x_{n+1} = x_n - \alpha_n \nabla f(x_n)</script></li>
<li><p>敏感性分析</p>
<script type="math/tex; mode=display">
f(x + \nabla x) \approx f'(x) \Delta x</script></li>
<li><p>机器学习</p>
<p>利用自动微分（back-propagation）来训练神经网络</p>
</li>
<li><p>求解非线性方程</p>
<p>Solving a nonlinear equation $f(x)=0$ using Newton’s method:</p>
<script type="math/tex; mode=display">
x_{n+1}= x_n - {f(x_n)\over f'(x_n)}</script></li>
</ul>
<h2 id="自动微分"><a href="#自动微分" class="headerlink" title="自动微分"></a>自动微分</h2><p>自动微分： a set of techniques to numerically evaluate the derivative of a function specified by a computer program (Wikipedia). It also bears other names such as <strong>autodiff, algorithmic differentiation, computational differentiation, and back-propagation</strong>.</p>
<p>目前可用的自动微分求解器：</p>
<ul>
<li><p>Tensorflow：python，static</p>
</li>
<li><p>Pytorch： python， dynamic</p>
</li>
<li><p>autograd：基于numpy</p>
</li>
<li><p>Adept-2: C++</p>
</li>
<li><p>ForwardDiff， Zygote ：Julia</p>
<p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/04/3jNPptEfx6HWdBZ.png" alt="3jNPptEfx6HWdBZ"></p>
</li>
</ul>
<p>上图是微软做的一个对比: 详细代码参考 <a target="_blank" rel="noopener" href="https://github.com/microsoft/ADBench">https://github.com/microsoft/ADBench</a></p>
<p>一个可微分的计算模型可以描述为 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=f+%3A+%5Cmathbb%7BR%7D%5Em+%5Crightarrow+%5Cmathbb%7BR%7D%5En" alt="[公式]"> 。考虑一个计算过程</p>
<p><img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Cbegin%7Balign%2A%7D+++++%26%5Cmathbf%7Bx%7D%5E1+%3D+f_1%28%5Cmathbf%7Bx%7D%5E0%29%5C%5C+++++%26%5Cmathbf%7Bx%7D%5E2+%3D+f_2%28%5Cmathbf%7Bx%7D%5E1%29%5C%5C+++++%26%5Cldots%5C%5C+++++%26%5Cmathbf%7Bx%7D%5EL+%3D+f_L%28%5Cmathbf%7Bx%7D%5E%7BL-1%7D%29+%5Cend%7Balign%2A%7D" alt="[公式]"></p>
<p>其中 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5E0%5Cin+R%5Em" alt="[公式]"> ， <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Cmathbf%7Bx%7D%5EL%5Cin+R%5En" alt="[公式]"> ， <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=L" alt="[公式]"> 是计算过程的“深度”。这个计算过程的一阶导数是一个大小为 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=n%5Ctimes+m" alt="[公式]"> 的Jacobian矩阵 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=J_%7Bij%7D+%5Cequiv+%5Cfrac%7B%5Cpartial+x%5EL_i%7D%7B%5Cpartial+x_j%5E0%7D" alt="[公式]"> ，其中 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=x_j%5E0" alt="[公式]"> 和 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=x_i%5EL" alt="[公式]"> 分别是输入和输出中的一个元素。</p>
<p>tangent mode AD可以在一次程序计算中通过链式法则 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cmathbf%7Bx%7D%5Ek%7D%7B%5Cpartial+x%5E0_j%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cmathbf%7Bx%7D%5Ek%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D%5E%7Bk-1%7D%7D%5Cfrac%7B%5Cpartial+%5Cmathbf%7Bx%7D%5E%7Bk-1%7D%7D%7B%5Cpartial+x%5E0_j%7D" alt="[公式]"> 递推得到Jacobian矩阵中与单个输入有关的部分，或者说是Jacobian矩阵的一列（图一）。这种AD实现起来很简单 [Revels2016]，也不需要很多额外的内存空间。但是它在变分优化中用的并不多，因为在变分算法中，Loss只有一个但可变分的参数很多，比如机器学习模型中的参数个数经常有 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Csim10%5E6" alt="[公式]"> 数量级之多。对于每个输入tangent mode AD都需要遍历计算过程以得到它的导数，重复遍历计算过程 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Csim10%5E6" alt="[公式]"> 次显然是无法接受的，于是86年Hinton提出了用后向传播技术训练神经网络 [Rumelhart1986]，也就是接下来要说的adjoint mode AD。</p>
<p><img src= "/img/loading.gif" data-src="https://pic2.zhimg.com/80/v2-58b646dec1ee0eb3677decab30662449_1440w.jpg" alt="img"></p>
<p>图一：tangent model AD和adjoint mode AD分别计算了Jacobian矩阵的一列和一行。（引自 [Hascoet2013]）</p>
<p>adjoint mode AD利用链式法则 <img src= "/img/loading.gif" data-src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+x%5EL_i%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D%5E%7Bk%7D%7D+%3D+%5Cfrac%7B%5Cpartial+x%5EL_i%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D%5E%7Bk%2B1%7D%7D%5Cfrac%7B%5Cpartial+%5Cmathbf%7Bx%7D%5E%7Bk%2B1%7D%7D%7B%5Cpartial+%5Cmathbf%7Bx%7D%5Ek%7D" alt="[公式]"> 可以仅通过一次对计算过程的遍历得到Jacobian矩阵的一行。但它的导数链式法则传递方向和程序执行方向相反，所以需要在程序计算过程中记录一些额外的信息来辅助求导，这些辅助信息包括计算图和计算过程的中间变量。计算图是一个有向无环图（DAG），它表达了函数和变量的关系。在现今主流的自动微分框架<a href="https://link.zhihu.com/?target=https%3A//pytorch.org/">Pytorch</a> [Paszke2017]和<a href="https://link.zhihu.com/?target=https%3A//github.com/FluxML/Flux.jl">Flux.jl</a> [Innes2018]中，程序在执行的时候把计算图信息记录在一个叫做“张量”的变量中。每个张量都有一个叫做tracker的结构体（实现细节见 </p>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/people/e3ab67ef7b41bb8f2b34ec5dbc0c811d">@罗秀哲</a>的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47592565">一天实现自己的自动微分</a>），它记录了产生这个张量的过程，包括函数和函数的输入（父节点）。在<a href="https://link.zhihu.com/?target=https%3A//www.tensorflow.org/">TensorFlow</a> [Tensorflow2015]中，用户必须在执行前构造静态图，而中间结果也被记录在这个静态图中。Julia下的源到源的自动微分（实现细节见 <a target="_blank" rel="noopener" href="https://www.zhihu.com/people/e3ab67ef7b41bb8f2b34ec5dbc0c811d">@罗秀哲</a> 的<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/75916086">一天实现你自己的源到源自动微分</a>）工具<a href="https://link.zhihu.com/?target=https%3A//github.com/FluxML/Zygote.jl">Zygote</a> [Innes2018, Innes2019]用程序的static single assignment (SSA) 这种中间表示当做计算图，中间变量存放在全局的链表中。</p>
<p>抄自 知乎</p>
<h2 id="Dual-Number"><a href="#Dual-Number" class="headerlink" title="Dual Number"></a>Dual Number</h2><script type="math/tex; mode=display">
{d\over dx}f(x) = lim_{\epsilon\rightarrow 0} {f(x+\epsilon)- f(x)\over \epsilon}</script><p>Let’s say that </p>
<script type="math/tex; mode=display">
f(x)= sin(x^2)</script><script type="math/tex; mode=display">
f(x+\epsilon)= sin((x+\epsilon)^2)\\\\
= sin(x^2 + 2x\epsilon + \epsilon^2)\\\\
= sin(x^2 + 2x\epsilon)\\\\
= sin(x^2)cos(3x\epsilon) + cos(x^2)sin(2x\epsilon)\\\\
= sin(x^2) + 2xcos(x^2)\epsilon</script><p>通过这个例子，我们发现，其微分可以直接被计算出来。</p>
<h2 id="Tracing"><a href="#Tracing" class="headerlink" title="Tracing"></a>Tracing</h2><p>“static declaration”: Theano/Tensorflow</p>
<p>Using julia’s operator overloading; the idea is to create a new type.</p>
<p>“Eager execution”: Chainer/PyTorj/Flux</p>
<p>just re-build the Wengert list from scratch every time!</p>
<h2 id="实际应用"><a href="#实际应用" class="headerlink" title="实际应用"></a>实际应用</h2><p>举一个简单的PDE的例子：</p>
<script type="math/tex; mode=display">
{\partial u (x, t)\over \partial t}= \kappa (x) \Delta u(x,t)+ f(x, t), t\in (0,T), x\in \Omega\\\\
u(1, t) = 0, t>0\\\\
\kappa(x){\partial u (0,t)\over \partial x}= 0 , t> 0,\\\\
suppose : \kappa(x) = a + bx, solve :a \& b</script><p><img src= "/img/loading.gif" data-src="/Users/wjq/Library/Application Support/typora-user-images/image-20210104113632199.png" alt="image-20210104113632199" style="zoom:50%;" /></p>
<p>This problem is a standard inverse problem. We can formulate the problem as a PDE-constrained optimization problem.</p>
<script type="math/tex; mode=display">
min_{a,b} \int_0 ^t (u(0,t)-u_0(t))^2 dt\\\\</script><p>First, we need to discrete the PDE operator, such as using central difference.</p>
<p>In this way, we can rewrite the equation as a linear system, we have:</p>
<script type="math/tex; mode=display">
A(a,b) U^{k+1} = U^k + F^{k+1}, U^k = \begin{bmatrix} u_1^k\\  u_2^k \\ \vdots \\ u_n^k \end{bmatrix}</script><p>Here, $\lambda_i = -k_i {\Delta t \over \Delta x^2}$ and</p>
<script type="math/tex; mode=display">
A(a,b)=\begin{bmatrix} 2\lambda_1 + 1 & -2 \lambda_1 & & &\\ -\lambda_2 & 2\lambda_2+1 & -\lambda_2 & &    \\ & -\lambda_3 & 2 \lambda_3 +1 & -\lambda_3 & & \\ & & \ddots & & \\ & & & & -\lambda_{n-1}\\ & & &  -\lambda_n  & 2\lambda_n +1 \end{bmatrix}</script><script type="math/tex; mode=display">
F^k = \Delta t \begin{bmatrix} f_1^{k+1}\\  f_2^{k+1} \\ \vdots \\ f_n^{k+1} \end{bmatrix}</script><p>In a word, our discretized optimization problem is :</p>
<script type="math/tex; mode=display">
min_{a,b} \sum_{k=1}^m (u_1^k - u_0((k-1)\Delta t))^2\\\\
s.t. A(a,b)U^{k+1} = U^k + F^{k+1}, k=1,2,...,m\\\\
U^0 = 0</script><p><img src= "/img/loading.gif" data-src="https://i.loli.net/2021/01/04/4uDko7bQq6AhlyZ.png" alt=""></p>
<p>事实上，我们可以用计算图来想象上述离散过程：</p>
<p>U通过线性算子 A和F不断随时间叠加，a，b参数就嵌在A里面，然后我们需要与测的数据进行对比，即所谓的训练过程，Loss输出</p>
<p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/av669322256/">https://www.bilibili.com/video/av669322256/</a></p>
<p>参考：</p>
<p> <a target="_blank" rel="noopener" href="https://github.com/Roger-luo/YAAD.jl">https://github.com/Roger-luo/YAAD.jl</a></p>
<p><a target="_blank" rel="noopener" href="https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6">https://marksaroufim.medium.com/automatic-differentiation-step-by-step-24240f97a6e6</a></p>
<p><a target="_blank" rel="noopener" href="https://github.com/MikeInnes/diff-zoo">https://github.com/MikeInnes/diff-zoo</a></p>
<p><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/47592565">https://zhuanlan.zhihu.com/p/47592565</a></p>
<p>I was playing with <a target="_blank" rel="noopener" href="https://github.com/denizyuret/AutoGrad.jl">AutoGrad.jl</a> and <a target="_blank" rel="noopener" href="https://github.com/FluxML/Zygote.jl">Zygote.jl</a>, they both look<br>awesome, and AutoGrad.jl has already been applied to the machine learning framework in Julia: <a target="_blank" rel="noopener" href="https://github.com/denizyuret/Knet.jl">Knet.jl</a>. When I tried to read the source code of AutoGrad.jl, it is actually quite simple and small.</p>
<p>But, as a PyTorch contributor and user, I personally prefer some of PyTorch’s interfaces (both frontend and backend), and as a Julian, I want to see how simple it can be to write a Julia AD package. Therefore, I tried to implemented my own automatic differentiation and it just took me one day to finished the core part (including broadcast!).</p>
<p>Although, I spent a few hours more during the next following days to polish the interface (a weekend to write a blog post). But it is actually quite easy to implement an automatic differentiation package in Julia.</p>
<p>I packed it to a package (YAAD.jl: Yet Another AD package for Julia) here: <a target="_blank" rel="noopener" href="https://github.com/Roger-luo/YAAD.jl">Roger-luo/YAAD.jl</a></p>
<p>In this post, I’ll introduce how did I implemented my own automatic differentiation, and maybe, you can build one of your own as well!</p>
<h2 id="Automatic-Differentiation-A-Brief-Intro"><a href="#Automatic-Differentiation-A-Brief-Intro" class="headerlink" title="Automatic Differentiation: A Brief Intro"></a>Automatic Differentiation: A Brief Intro</h2><p>There are generally two kinds of automatic differentiation: forward mode differentiation and reverse mode differentiation. What we need in deep learning (as well as tensor networks in physics) is the reverse mode differentiation, because the model we are going to optimize usually contains quite a lot parameters. This is also called as back-propagation and requires something called comput-graph.</p>
<h3 id="Comput-Graph"><a href="#Comput-Graph" class="headerlink" title="Comput-Graph"></a>Comput-Graph</h3><p><em>To illustrate this, I stole some nice picture and re-ploted them in animation from cs5740, 2017sp, Cornell.</em></p>
<p>Say we are calculating the following expression:</p>
<p>y=xTAx+b⋅x+c</p>
<p>We will need to call several functions in Julia to get the result y, which is</p>
<ol>
<li>z1=xT: <code>transpose</code> function.</li>
<li>z2=z1A matrix-vector multiplication, which can be <code>gemv</code> in <code>LinearAlgebra.BLAS</code>, or just <code>*</code>.</li>
<li>y1=z2x vector dot operation, which is <code>LinearAlgebra.dot</code> or the UTF-8 operator <code>x ⋅ y</code></li>
<li>y2=b⋅x another vector dot</li>
<li>y1+y2+c a scalar add function, one can calculate it by simply calling <code>+</code> operator in Julia.</li>
</ol>
<p>In fact, we can draw a graph of this expression, which illustrates the relationship between each variable in this expression.<br>Each node in the graph with an output arrow represents a variable and each node with an input arrow represents a function/operator.</p>
<p><img src= "/img/loading.gif" data-src="https://blog.rogerluo.me/images/comput-graph-forward.gif" alt="comput-graph"></p>
<p>The evaluation of the math equation above can then be expressed as a process called <strong>forward evaluation</strong>, it starts from the leaf nodes, which represents the inputs of the whole expression, e.g they are x,A,b,c in our expression. Each time, we receive the value of a node in the graph, we mark the node with <strong>green</strong>.</p>
<p>Now, let’s calculate the gradients with <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Chain_rule"><strong>chain rule</strong></a>, the number of gradients returned by each function is the same as their inputs. We mark the node red if we receive a gradient, the gradient will be back propagated through the graph, which is called <strong>back propagation</strong> or <strong>backward evaluation</strong>.</p>
<p><img src= "/img/loading.gif" data-src="https://blog.rogerluo.me/images/comput-graph-backward.gif" alt="comput-graph"></p>
<h3 id="Dynamic-Comput-Graphs-VS-Static-Comput-Graphs"><a href="#Dynamic-Comput-Graphs-VS-Static-Comput-Graphs" class="headerlink" title="Dynamic Comput Graphs VS Static Comput Graphs"></a>Dynamic Comput Graphs VS Static Comput Graphs</h3><p>Although, the way of forward evaluation and backward evaluation are actually the same, but for implementation, we can construct the graph on the fly (like in <a target="_blank" rel="noopener" href="https://github.com/pytorch/pytorch">PyTorch</a>) or as a static declaration (like in <a target="_blank" rel="noopener" href="https://github.com/tensorflow/tensorflow">TensorFlow</a>).</p>
<p>Generally, the difference between them is that:</p>
<p><strong>Whether the graph is defined before the forward evaluation happens or along with the forward evaluation.</strong></p>
<p>I’m a PyTorch syntax lover, so I’m going to implement my AD as a dynamic constructed graph. But I’m also planning to write a macro in Julia that “freeze” a dynamic graph to static graph, because in principle, static graph is easier to optimize, since we will be able to access the whole graph before evaluation happens, which allows us to dispatch methods statically, but static graphs can be hard to debug.</p>
<h2 id="Define-the-Nodes-in-the-Computational-Graph"><a href="#Define-the-Nodes-in-the-Computational-Graph" class="headerlink" title="Define the Nodes in the Computational Graph"></a>Define the Nodes in the Computational Graph</h2><p>Well, before we start writing something concrete, we can first define an <code>abstract type</code> for all nodes we are going to define:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">abstract type AbstractNode end</span><br></pre></td></tr></table></figure>
<h3 id="Leaf-Nodes-可能是中间节点"><a href="#Leaf-Nodes-可能是中间节点" class="headerlink" title="Leaf Nodes 可能是中间节点"></a>Leaf Nodes 可能是中间节点</h3><p>Same, define an <code>abstract type</code> first.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">abstract type LeafNode &lt;: AbstractNode end</span><br></pre></td></tr></table></figure>
<p>In PyTorch, a <code>Variable</code> is a multi-dimensional array (tensor) with a gradient (also store in a multi-dimensional array of the same size and data type)—-DUAL NUMBER. And it will accumulate the gradient if we back-propagate the graph for multiple times-AD思想.</p>
<p>Accumulating is sometimes useful, when you want to calculate the expectation of the gradient, or manipulate a batch of data, but not always useful. But anyway, we have an abstract type, we can define different flavored leaf nodes later.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mutable struct Variable&#123;T&#125; &lt;: LeafNode</span><br><span class="line">    value::T</span><br><span class="line">    grad::T</span><br><span class="line"></span><br><span class="line">    Variable(val::T) where T &#x3D; new&#123;T&#125;(val)</span><br><span class="line">    Variable(val::T, grad::T) where T &#x3D; new&#123;T&#125;(val)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Here, we use <a target="_blank" rel="noopener" href="https://docs.julialang.org/en/v1/manual/constructors/#Incomplete-Initialization-1">incomplete initialization</a>, since we don’t really need to allocate a memory for the gradient at the beginning, we can just take the ownership of a temporary variable’s memory later.</p>
<h3 id="Other-Nodes"><a href="#Other-Nodes" class="headerlink" title="Other Nodes"></a>Other Nodes</h3><p>Well, now we have some leaf nodes, but we need to <strong>store operations and their output for later use,</strong> so firstly, I define something called <code>Node</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Node&#123;FT &lt;: Function, ArgsT &lt;: Tuple, KwargsT &lt;: NamedTuple&#125; &lt;: AbstractNode</span><br><span class="line">    f::FT # function</span><br><span class="line">    args::ArgsT #Tuple</span><br><span class="line">    kwargs::KwargsT # NameTuple</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>It is a subtype of <code>AbstractNode</code>, and it stores a function call’s arguments and keywords. However, we will need to consider<br><code>broadcast</code> and normal function calls, they are actually different, therefore we should not directly store the function, thus, so let’s write some <code>traits</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">abstract type Operator end</span><br><span class="line"></span><br><span class="line">module Trait</span><br><span class="line">import YAAD: Operator</span><br><span class="line"></span><br><span class="line">struct Method&#123;FT&#125; &lt;: Operator</span><br><span class="line">    f::FT</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">struct Broadcasted&#123;FT&#125; &lt;: Operator</span><br><span class="line">    f::FT</span><br><span class="line">end</span><br><span class="line">end # Trait</span><br></pre></td></tr></table></figure>
<p>Now we change <code>Function</code> to <code>Operator</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">struct Node&#123;FT &lt;: Operator, ArgsT &lt;: Tuple, KwargsT &lt;: NamedTuple&#125; &lt;: AbstractNode</span><br><span class="line">    f::FT</span><br><span class="line">    args::ArgsT</span><br><span class="line">    kwargs::KwargsT</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>And we may make some constructors for convenience, since most <code>f</code>s will be method calls rather than broadcasts or self-defined<br>operators, and we usually don’t need the keyword arguments either:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># wrap function to Method</span><br><span class="line">Node(f::Function, args, kwargs) &#x3D; Node(Trait.Method(f), args, kwargs)</span><br><span class="line">Node(op, args) &#x3D; Node(op, args, NamedTuple())</span><br></pre></td></tr></table></figure>
<p>In fact, <code>Node</code> is actually just a trait for some object (some subtype of <code>Operator</code>), we haven’t<br>defined the type that store the output of each node in the graph, so here let’s define a <code>CachedNode</code><br>which will cache the forward evaluation result of <code>Node</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">mutable struct CachedNode&#123;NT &lt;: AbstractNode, OutT&#125; &lt;: AbstractNode</span><br><span class="line">    node::NT</span><br><span class="line">    output::OutT</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>So, to store the forward evaluation result of a <code>Node</code> with <code>CachedNode</code> when it is constructed, we need to forward propagate<br>the comput-graph recorded in <code>Node</code> and assign it to the cache: 前传</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function CachedNode(f, args...; kwargs...)</span><br><span class="line">    node &#x3D; Node(f, args, kwargs.data) # this constructs a Node</span><br><span class="line">    output &#x3D; forward(node)</span><br><span class="line">    CachedNode(node, output)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h2 id="Evaluations"><a href="#Evaluations" class="headerlink" title="Evaluations"></a>Evaluations</h2><p>The evaluation is the most important part, because we want to define our rules of evaluation in an extensible way, and<br>try to make it efficient. Luckily, in Julia, we have <strong>multiple dispatch</strong>! Let’s make use of it!</p>
<h3 id="Forward-Evaluation"><a href="#Forward-Evaluation" class="headerlink" title="Forward Evaluation"></a>Forward Evaluation</h3><p>But how do we <strong>forward evaluate</strong> a <code>Node</code>? This depends on what kind of method is implemented for this generic function <code>forward</code>:</p>
<ol>
<li>If input is a <code>Node</code>, we re-dispatch this method to its operator’s forward method (while it forward evaluates the <code>args</code> and <code>kwargs</code>):</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward(node::Node) &#x3D; forward(node.f, map(forward, node.args)...; map(forward, node.kwargs)...)</span><br></pre></td></tr></table></figure>
<p>This will allow us to tweak the forward evaluation by simply implementing a method for the generic function <code>forward</code>, e.g, if we don’t want to directly calculate the result of a linear operator Wx+b rather than store two nodes separately (a matrix-vector multiplication <code>*</code> and an add function <code>+</code>).</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">struct Linear &lt;: Operator</span><br><span class="line">  w::Matrix&#123;Float64&#125;</span><br><span class="line">  b::Vector&#123;Float64&#125;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">forward(op::Linear, x::Vector&#123;Float64&#125;) &#x3D; op.w * x + b</span><br></pre></td></tr></table></figure>
<ol>
<li>If input is a <code>CachedNode</code>, this means our user is evaluating this node for the second time (since we calculate the result when construct it), we will update its output</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward(node::CachedNode) &#x3D; (node.output &#x3D; forward(node.node))</span><br></pre></td></tr></table></figure>
<ol>
<li>However, for simple function calls, we don’t want to write something like</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">function forward(::Method&#123;typeof(sin)&#125;, x)</span><br><span class="line">  sin(x)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>each time, let’s make it simpler, by re-dispatching an operator’s <code>forward</code> method to a function call:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward(op::Operator, args...; kwargs...) &#x3D; op.f(args...; kwargs...)</span><br></pre></td></tr></table></figure>
<p>This means, as long as, the operator defines its own call method, it does not need to implement a method for <code>forward</code>, e.g</p>
<p>We can just define the call method for <code>Linear</code> rather than defining a method for <code>forward</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">(op::Linear)(x::Vector) &#x3D; op.w * x + b</span><br></pre></td></tr></table></figure>
<ol>
<li>There could be some constants in the <code>Node</code>, e.g when we call <code>Variable(2.0) + 1.0</code>, this <code>1.0</code> is actually a constant, therefore, we can just return it, when the input is not part of the computational graph (not a subtype of <code>AbstractNode</code>) and define a default method for <code>AbstractNode</code> for better error messages.</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">forward(x) &#x3D; x</span><br><span class="line">forward(x::NT) where &#123;NT &lt;: AbstractNode&#125; &#x3D; error(&quot;forward method is not implemented for node type: $NT&quot;)</span><br></pre></td></tr></table></figure>
<ol>
<li>For leaf nodes, they should directly return their value, but we might use another kind of leaf node to make the non-PyTorch lover<br>happy in the future, so let’s define a generic function <code>value</code> to get this property:</li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">value(x) &#x3D; x</span><br><span class="line"></span><br><span class="line">function value(x::T) where &#123;T &lt;: AbstractNode&#125;</span><br><span class="line">    error(</span><br><span class="line">        &quot;Expected value in this node $x of type $T &quot;,</span><br><span class="line">        &quot;check if you defined a non-cached node&quot;,</span><br><span class="line">        &quot; or overload value function for your node.&quot;</span><br><span class="line">    )</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">value(x::Variable) &#x3D; x.value</span><br><span class="line">value(x::CachedNode) &#x3D; value(x.output)</span><br></pre></td></tr></table></figure>
<p>And leaf nodes’ <code>forward</code> directly return its value:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">forward(node::LeafNode) &#x3D; value(node)</span><br></pre></td></tr></table></figure>
<p>Okay! We have defined all we need for <code>forward</code> evaluation, now let’s try to implement backward evaluation.</p>
<h3 id="Backward-Evaluation"><a href="#Backward-Evaluation" class="headerlink" title="Backward Evaluation"></a>Backward Evaluation</h3><p>The backward evaluation is actually similar to forward evaluation, we will call backward recursively on each node and its <code>args</code> (no, I’m not going to support <code>backward</code> on <code>kwargs</code> here, XD).</p>
<p>Firstly, for <code>LeafNode</code>, this is simple, e.g <code>Variable</code> will just take the <code>grad</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">function backward(x::Variable, grad)</span><br><span class="line">    if isdefined(x, :grad)</span><br><span class="line">        x.grad +&#x3D; grad</span><br><span class="line">    else</span><br><span class="line">        x.grad &#x3D; grad</span><br><span class="line">    end</span><br><span class="line">    nothing</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>We will check if this <code>grad</code> member is defined (it is incomplete initialized!), if it is not, we will just use the memory of<br>this gradient, or we can add it to the current gradient, just like PyTorch’s <code>Variable</code> (or <code>Tensor</code> after v0.4).</p>
<p>And now, we need to define how to backward evaluate a <code>CachedNode</code>:</p>
<ol>
<li>We gather the gradients of inputs from a function called <code>gradient</code></li>
<li>We put each corresponding gradient to sub-node of current node and call their <code>backward</code></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function backward(node::CachedNode, f, grad)</span><br><span class="line">    grad_inputs &#x3D; gradient(node, grad)</span><br><span class="line">    for (each, each_grad) in zip(args(node), grad_inputs)</span><br><span class="line">        backward(each, each_grad)</span><br><span class="line">    end</span><br><span class="line">    nothing</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Oh, you might want to add some assertion to output a better error message here, we will check the type of gradient and output and also their size here, in most cases, gradient should have the exact same<br>type and size as output:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">backward_type_assert(node::CachedNode&#123;&lt;:AbstractNode, T&#125;, grad::T) where T &#x3D; true</span><br><span class="line">backward_type_assert(node::CachedNode&#123;&lt;:AbstractNode, T1&#125;, grad::T2) where &#123;T1, T2&#125; &#x3D;</span><br><span class="line">    error(&quot;Gradient is expected to have the same&quot;,</span><br><span class="line">          &quot; type with outputs, expected $T1&quot;,</span><br><span class="line">          &quot; got $T2&quot;)</span><br></pre></td></tr></table></figure>
<p>but for subtype of <code>AbstractArray</code>, we can just allow them to have the same static parameter (tensor rank and data type), because we will probably be dealing with <code>SubArray</code> and <code>Array</code> for some operators, which does not really matters</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># exclude arrays</span><br><span class="line">backward_type_assert(node::CachedNode&#123;&lt;:AbstractNode, T1&#125;, grad::T2) where</span><br><span class="line">    &#123;T, N, T1 &lt;: AbstractArray&#123;T, N&#125;, T2 &lt;: AbstractArray&#123;T, N&#125;&#125; &#x3D; true</span><br></pre></td></tr></table></figure>
<p>Finally we check the size of the gradients and outputs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function backward_size_assert(node::CachedNode, grad)</span><br><span class="line">    size(node.output) &#x3D;&#x3D; size(grad) ||</span><br><span class="line">        error(</span><br><span class="line">            &quot;gradient should have the same size with output,&quot;,</span><br><span class="line">            &quot; expect size $(size(node.output)), got $(size(grad))&quot;</span><br><span class="line">        )</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>In Julia, there is a compiler option to turn bounds check off. We sometimes don’t actually need to check bounds at runtime<br>so we put this assertion in <code>@boundscheck</code>. It looks like:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">function backward(node::CachedNode, f, grad)</span><br><span class="line">    backward_type_assert(node, grad)</span><br><span class="line">    # TODO: replace with @assert when there is a compiler option for it</span><br><span class="line">    @boundscheck backward_size_assert(node, grad)</span><br><span class="line"></span><br><span class="line">    grad_inputs &#x3D; gradient(node, grad)</span><br><span class="line">    for (each, each_grad) in zip(args(node), grad_inputs)</span><br><span class="line">        backward(each, each_grad)</span><br><span class="line">    end</span><br><span class="line">    nothing</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>OK, now, let’s think about how to return the gradient, I would prefer our AD be highly extensible by taking advantage of Julia’s <strong>multiple dispatch</strong>, and I will only need to define the gradient by defining different methods for <code>gradient</code>, e.g</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradient(::typeof(sin), grad, output, x) &#x3D; grad * cos(x)</span><br></pre></td></tr></table></figure>
<p>This can be implemented in the same way as <code>forward</code>: re-dispatch the method to different syntax:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">gradient(x::CachedNode, grad) &#x3D; gradient(x.node.f, grad, x.output, map(value, x.node.args)...; map(value, x.node.kwargs)...)</span><br></pre></td></tr></table></figure>
<p>Here we dispatch the <code>gradient</code> of a <code>CachedNode</code> directly to a method implemented for <code>Operator</code>, but we have the same situation with <code>forward</code>, we don’t want to write <code>Method</code> trait each time</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">gradient(x::Operator, grad, output, args...; kwargs...) &#x3D;</span><br><span class="line">    gradient(x.f, grad, output, args...; kwargs...)</span><br></pre></td></tr></table></figure>
<p>Finally, define a default error massage:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">gradient(fn, grad, output, args...; kwargs...) &#x3D;</span><br><span class="line">    error(</span><br><span class="line">        &quot;gradient of operator $fn is not defined\n&quot;,</span><br><span class="line">        &quot;Possible Fix:\n&quot;,</span><br><span class="line">        &quot;define one of the following:\n&quot;,</span><br><span class="line">        &quot;1. gradient(::typeof($fn), grad, output, args...; kwargs...)\n&quot;,</span><br><span class="line">        &quot;2. gradient(op::Trait.Method&#123;typeof($fn)&#125;, grad, output, args...; kwargs...)\n&quot;,</span><br><span class="line">        &quot;3. gradient(op::Trait.Broadcasted&#123;typeof($fn)&#125;, grad, output, args...; kwargs...)\n&quot;</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>So in this way, when we implement a specific method of some types for <code>gradient</code>, Julia will auto dispatch gradient to that method, e.g</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># I re-define the concrete type &#96;Linear&#96; here in order to store the gradient</span><br><span class="line">struct Linear &lt;: Operator</span><br><span class="line">  w::Variable&#123;Matrix&#123;Float64&#125;&#125;</span><br><span class="line">  b::Variable&#123;Vector&#123;Float64&#125;&#125;</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function gradient(op::Linear, grad, output, x)</span><br><span class="line">  grad_w, grad_b &#x3D; # some gradient expression to calculate the gradient of w and b</span><br><span class="line">  backward(op.w, grad_w) # update gradient of w</span><br><span class="line">  backward(op.w, grad_b) # update gradient of b</span><br><span class="line"></span><br><span class="line">  grad_input &#x3D; # calculate the gradient of input</span><br><span class="line">  grad_input # return the gradient of input</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Umm, and finally, I would like to have an eye-candy function to construct a node (but this depends on you, it is not actually necessary):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">register(f, args...; kwargs...) &#x3D; CachedNode(f, args...; kwargs...)</span><br></pre></td></tr></table></figure>
<p>Okay, let’s try to register an operator now!</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Base.sin(x::AbstractNode) &#x3D; register(Base.sin, x)</span><br><span class="line">gradient(::typeof(Base.sin), grad, output, x) &#x3D; (grad * cos(x), )</span><br></pre></td></tr></table></figure>
<p><strong>Remember we assumed gradient returns several gradients</strong>, the return of <code>gradient</code> has to be an iteratable of gradients.</p>
<h2 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h2><p>However, for above gradients for scalars, this will just work. It won’t work for arrays. We will need to re-dispatch broadcast in Julia.</p>
<p>Let me introduce some basic concepts of the interface of broadcast in Julia first, and then we will find a very easy way<br>to implement AD for broadcast:</p>
<p>The whole broadcast mechanism is implemented in a module <code>Broadcast</code> in <code>Base</code>, each different type has its own <code>BroadcastStyle</code> (this is a trait). So what we need to do, is just to implement our own broadcast style and construct a<br><code>CachedNode</code> instead directly broadcasting the operation.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">struct ComputGraphStyle &lt;: Broadcast.BroadcastStyle end</span><br><span class="line">Base.BroadcastStyle(::Type&#123;&lt;:AbstractNode&#125;) &#x3D; ComputGraphStyle()</span><br><span class="line">Broadcast.BroadcastStyle(s::ComputGraphStyle, x::Broadcast.BroadcastStyle) &#x3D; s</span><br></pre></td></tr></table></figure>
<p>However, this is not enough, in Julia broadcast is lazy-evaluated, which can fuse broadcast and provide better performance, we need to re-dispatch two interface: <code>broadcasted</code> and <code>materialize</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">function Broadcast.broadcasted(::ComputGraphStyle, f, args...)</span><br><span class="line">    mt &#x3D; Trait.Broadcasted(f)</span><br><span class="line">    register(mt, args...)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">Broadcast.materialize(x::AbstractNode) &#x3D; register(Broadcast.materialize, x)</span><br></pre></td></tr></table></figure>
<p>And we let <code>materialize</code> directly return the gradient during backward evaluation:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">function backward(node::CachedNode, ::typeof(Broadcast.materialize), grad)</span><br><span class="line">    backward_type_assert(node, grad)</span><br><span class="line">    @boundscheck backward_size_assert(node, grad)</span><br><span class="line">    backward(node.node.args[1], grad) # materialize only has one arguments, we don&#39;t need the for loop</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Now, if you try to broadcast with this AD, you would find that the assertion we defined in <code>backward</code> is quite annoying (because lazy evaluation, its output is not actually the real output, but a middle type), let’s mute them for broadcast:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function backward(node::CachedNode, ::Trait.Broadcasted, grad)</span><br><span class="line">    grad_inputs &#x3D; gradient(node, grad)</span><br><span class="line">    for (each, each_grad) in zip(args(node), grad_inputs)</span><br><span class="line">        backward(each, each_grad)</span><br><span class="line">    end</span><br><span class="line">    nothing</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<h2 id="Add-more-operators-for-FREE"><a href="#Add-more-operators-for-FREE" class="headerlink" title="Add more operators for FREE!"></a>Add more operators for FREE!</h2><p>There is a Julia package called <code>DiffRules</code>, it contains quite a lot differentiation rules defined as Julia <code>Expr</code>, so we can just use code generation to generate operators with it rather than define them ourselves:</p>
<p>The rules are in <code>DiffRules.DEFINED_DIFFRULES</code>, so we will just iterate through its key</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">for (mod, name, nargs) in keys(DiffRules.DEFINED_DIFFRULES)</span><br><span class="line">  # code generation</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>the first argument <code>mod</code> is the module’s name, like for <code>sin</code>, it is actually in <code>Base</code>, so the <code>mod</code> is <code>Base</code> and<br><code>name</code> is the function’s name, <code>nargs</code> means the number of arguments, in <code>DiffRules</code>, there are only single argument functions<br>and double arguments functions.</p>
<p>So the code generation will look like</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">for (mod, name, nargs) in keys(DiffRules.DEFINED_DIFFRULES)</span><br><span class="line">    f_ex_head &#x3D; Expr(:., mod, QuoteNode(name))</span><br><span class="line"></span><br><span class="line">    if nargs &#x3D;&#x3D; 1</span><br><span class="line">        df_ex &#x3D; DiffRules.diffrule(mod, name, :x)</span><br><span class="line"></span><br><span class="line">        name &#x3D;&#x3D;&#x3D; :abs &amp;&amp; continue # exclude abs, it cannot be directly broadcasted</span><br><span class="line"></span><br><span class="line">        @eval begin</span><br><span class="line">            $(f_ex_head)(x::AbstractNode) &#x3D; register($(f_ex_head), x)</span><br><span class="line">            gradient(::typeof($(f_ex_head)), grad, output, x) &#x3D; (grad * $df_ex, )</span><br><span class="line">            gradient(mt::Trait.Broadcasted&#123;typeof($f_ex_head)&#125;, grad, output, x) &#x3D; (@.(grad * $(df_ex)), )</span><br><span class="line">        end</span><br><span class="line">    elseif nargs &#x3D;&#x3D; 2</span><br><span class="line">        df_ex &#x3D; DiffRules.diffrule(mod, name, :x, :y)</span><br><span class="line"></span><br><span class="line">        @eval begin</span><br><span class="line"></span><br><span class="line">            $(f_ex_head)(x1::AbstractNode, x2) &#x3D; register($f_ex_head, x1, x2)</span><br><span class="line">            $(f_ex_head)(x1, x2::AbstractNode) &#x3D; register($f_ex_head, x1, x2)</span><br><span class="line">            $(f_ex_head)(x1::AbstractNode, x2::AbstractNode) &#x3D; register($f_ex_head, x1, x2)</span><br><span class="line"></span><br><span class="line">            gradient(::typeof($f_ex_head), grad, output, x, y) &#x3D;</span><br><span class="line">                (grad * $(df_ex[1]), grad * $(df_ex[2]))</span><br><span class="line">            gradient(::Trait.Broadcasted&#123;typeof($f_ex_head)&#125;, grad, output, x, y) &#x3D;</span><br><span class="line">                (@.(grad * ($(df_ex[1]))), @.(grad * $(df_ex[2])))</span><br><span class="line">        end</span><br><span class="line">    else</span><br><span class="line">        @info &quot;unknown operator $name&quot;</span><br><span class="line">    end</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>For how to use code generation in Julia, I would recommend the official documentation to get a better understanding of it: <a target="_blank" rel="noopener" href="https://docs.julialang.org/en/v1/manual/metaprogramming/#Code-Generation-1">Code Generation</a>. I escape <code>abs</code> here because the differentiation expression of <code>abs</code> generated by <code>DiffRules</code> can not be directly broadcasted by <code>@.</code> (this macro add a broadcast mark <code>.</code> to every function call), so I have to implement its gradient manually. But <code>DiffRules</code> will generate most of the math function’s gradient for you!</p>
<h2 id="Polish"><a href="#Polish" class="headerlink" title="Polish"></a>Polish</h2><p>We roughly implemented the core functionality of an AD, but there’s still quite a lot to do to make it look and feel better.</p>
<p>I defined better printing later here: <a target="_blank" rel="noopener" href="https://github.com/Roger-luo/YAAD.jl/blob/master/src/show.jl">show.jl</a>, the basic idea is to re-dispatch our nodes via several traits, so we can insert a type into another type tree, e.g as subtype of <code>AbstractArray</code> and then make use of existing printing methods.</p>
<p>Then, to implement unit tests, I copied the <code>gradcheck</code> function from <code>PyTorch</code>, which will calculate the jacobian of an operator with the AD package and compare it with the numerical jacobian.</p>
<h2 id="Benchmark"><a href="#Benchmark" class="headerlink" title="Benchmark"></a>Benchmark</h2><p>Okay, it is done! With only about 200~300 lines Julia, what can we get? Actually, I thought it would be just a toy, but<br>it is actually amazing, when I tried to use it for my own work:</p>
<p>So I need to calculate something called <a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Matrix_product_state">matrix product state</a>, well, I’m not going to talk about quantum physics, so in short, it is just some rank-3 tensors (3 dimensional array), and we will need to calculate something like the following expression:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tr(x1 * x2 * x3)</span><br></pre></td></tr></table></figure>
<p>where <code>x1</code>, <code>x2</code>, <code>x3</code> are just matrices.</p>
<p>So I implemented the gradient of <code>tr</code> and matrix multiplication:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Base.:(*)(lhs::AbstractNode, rhs) &#x3D; register(Base.:(*), lhs, rhs)</span><br><span class="line">Base.:(*)(lhs, rhs::AbstractNode) &#x3D; register(Base.:(*), lhs, rhs)</span><br><span class="line">Base.:(*)(lhs::AbstractNode, rhs::AbstractNode) &#x3D; register(Base.:(*), lhs, rhs)</span><br><span class="line"></span><br><span class="line">using LinearAlgebra</span><br><span class="line"></span><br><span class="line">LinearAlgebra.tr(x::AbstractNode) &#x3D; register(LinearAlgebra.tr, x)</span><br><span class="line">gradient(::typeof(tr), grad, output, x) &#x3D; (grad * Matrix(I, size(x)), )</span><br><span class="line"></span><br><span class="line">function gradient(::typeof(*), grad, output, lhs::AbstractVecOrMat, rhs::AbstractVecOrMat)</span><br><span class="line">    grad * transpose(rhs), transpose(lhs) * grad</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>Now let’s benchmark <code>tr(x1 * x2)</code> on the CPU with other packages, with the following function call</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">Zygote.@grad LinearAlgebra.tr(x) &#x3D; LinearAlgebra.tr(x), Δ-&gt; (Δ * Matrix(I, size(x)), )</span><br><span class="line"></span><br><span class="line">function bench_tr_mul_yaad(x1, x2)</span><br><span class="line">    z &#x3D; tr(x1 * x2)</span><br><span class="line">    YAAD.backward(z)</span><br><span class="line">    x1.grad, x2.grad</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function bench_tr_mul_autograd(x1, x2)</span><br><span class="line">    z &#x3D; AutoGrad.@diff tr(x1 * x2)</span><br><span class="line">    AutoGrad.grad(z, x1), AutoGrad.grad(z, x2)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function bench_tr_mul_zygote(x1, x2)</span><br><span class="line">    Zygote.gradient((x1, x2)-&gt;tr(x1 * x2), x1, x2)</span><br><span class="line">end</span><br><span class="line"></span><br><span class="line">function bench_tr_mul_flux(x1, x2)</span><br><span class="line">    z &#x3D; tr(x1 * x2)</span><br><span class="line">    back!(z, 1)</span><br><span class="line">    Tracker.grad(x1), Tracker.grad(x2)</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>and in PyTorch (our interface is quite similar to PyTorch, isn’t it?)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def bench_tr_mul_torch(x1, x2):</span><br><span class="line">    z &#x3D; torch.trace(torch.matmul(x1, x2))</span><br><span class="line">    z.backward()</span><br><span class="line">    return x1.grad, x2.grad</span><br></pre></td></tr></table></figure>
<p>In Julia, we use <code>BenchmarkTools</code> to measure the time, and in Python we can use the magic command <code>timeit</code> in ipython.</p>
<p>The value is defined as follows</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">xv, yv &#x3D; rand(30, 30), rand(30, 30)</span><br><span class="line">yaad_x, yaad_y &#x3D; YAAD.Variable(xv), YAAD.Variable(yv)</span><br><span class="line">autograd_x, autograd_y &#x3D; AutoGrad.Param(xv), AutoGrad.Param(yv)</span><br><span class="line">flux_x, flux_y &#x3D; Flux.param(xv), Flux.param(yv)</span><br></pre></td></tr></table></figure>
<p>Before we benchmark other packages, I also wrote a baseline function, which calculates the gradient manually:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">function bench_tr_mul_base(x1, x2)</span><br><span class="line">    z1 &#x3D; x1 * x2</span><br><span class="line">    z2 &#x3D; tr(z1)</span><br><span class="line"></span><br><span class="line">    grad_z1 &#x3D; Matrix&#123;eltype(z1)&#125;(I, size(z1))</span><br><span class="line">    grad_z1 * transpose(x2), transpose(x1) * grad_z1</span><br><span class="line">end</span><br></pre></td></tr></table></figure>
<p>And then tests it with <code>@benchmark</code>, which will run this function multiple times</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">julia&gt; @benchmark bench_tr_mul_autograd(autograd_x, autograd_y)</span><br><span class="line">BenchmarkTools.Trial:</span><br><span class="line">  memory estimate:  33.20 KiB</span><br><span class="line">  allocs estimate:  82</span><br><span class="line">  --------------</span><br><span class="line">  minimum time:     50.218 μs (0.00% GC)</span><br><span class="line">  median time:      62.364 μs (0.00% GC)</span><br><span class="line">  mean time:        90.422 μs (9.86% GC)</span><br><span class="line">  maximum time:     55.386 ms (99.86% GC)</span><br><span class="line">  --------------</span><br><span class="line">  samples:          10000</span><br><span class="line">  evals&#x2F;sample:     1</span><br><span class="line"></span><br><span class="line">julia&gt; @benchmark bench_tr_mul_yaad(yaad_x, yaad_y)</span><br><span class="line">BenchmarkTools.Trial:</span><br><span class="line">  memory estimate:  51.50 KiB</span><br><span class="line">  allocs estimate:  16</span><br><span class="line">  --------------</span><br><span class="line">  minimum time:     10.387 μs (0.00% GC)</span><br><span class="line">  median time:      13.429 μs (0.00% GC)</span><br><span class="line">  mean time:        24.273 μs (45.13% GC)</span><br><span class="line">  maximum time:     55.963 ms (99.96% GC)</span><br><span class="line">  --------------</span><br><span class="line">  samples:          10000</span><br><span class="line">  evals&#x2F;sample:     1</span><br><span class="line"></span><br><span class="line">julia&gt; @benchmark bench_tr_mul_zygote(xv, yv)</span><br><span class="line">BenchmarkTools.Trial:</span><br><span class="line">  memory estimate:  29.98 KiB</span><br><span class="line">  allocs estimate:  10</span><br><span class="line">  --------------</span><br><span class="line">  minimum time:     42.527 μs (0.00% GC)</span><br><span class="line">  median time:      46.640 μs (0.00% GC)</span><br><span class="line">  mean time:        56.996 μs (15.31% GC)</span><br><span class="line">  maximum time:     51.718 ms (99.90% GC)</span><br><span class="line">  --------------</span><br><span class="line">  samples:          10000</span><br><span class="line">  evals&#x2F;sample:     1</span><br><span class="line"></span><br><span class="line">julia&gt; @benchmark bench_tr_mul_base(xv, yv)</span><br><span class="line">BenchmarkTools.Trial:</span><br><span class="line">  memory estimate:  28.78 KiB</span><br><span class="line">  allocs estimate:  5</span><br><span class="line">  --------------</span><br><span class="line">  minimum time:     6.413 μs (0.00% GC)</span><br><span class="line">  median time:      8.201 μs (0.00% GC)</span><br><span class="line">  mean time:        12.215 μs (31.57% GC)</span><br><span class="line">  maximum time:     11.012 ms (99.87% GC)</span><br><span class="line">  --------------</span><br><span class="line">  samples:          10000</span><br><span class="line">  evals&#x2F;sample:     5</span><br><span class="line"></span><br><span class="line">julia&gt; @benchmark bench_tr_mul_flux(flux_x, flux_y)</span><br><span class="line">BenchmarkTools.Trial:</span><br><span class="line">  memory estimate:  30.25 KiB</span><br><span class="line">  allocs estimate:  24</span><br><span class="line">  --------------</span><br><span class="line">  minimum time:     8.009 μs (0.00% GC)</span><br><span class="line">  median time:      10.002 μs (0.00% GC)</span><br><span class="line">  mean time:        14.412 μs (30.14% GC)</span><br><span class="line">  maximum time:     16.286 ms (99.87% GC)</span><br><span class="line">  --------------</span><br><span class="line">  samples:          10000</span><br><span class="line">  evals&#x2F;sample:     3</span><br></pre></td></tr></table></figure>
<p>and for PyTorch (version v0.4.1)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">In [4]: x &#x3D; torch.rand(30, 30, dtype&#x3D;torch.float64, requires_grad&#x3D;True)</span><br><span class="line"></span><br><span class="line">In [5]: y &#x3D; torch.rand(30, 30, dtype&#x3D;torch.float64, requires_grad&#x3D;True)</span><br><span class="line"></span><br><span class="line">In [6]: %timeit bench_tr_mul_torch(x, y)</span><br><span class="line">76.8 µs ± 1.68 µs per loop (mean ± std. dev. of 7 runs, 10000 loops each)</span><br></pre></td></tr></table></figure>
<p>Our implementation is not bad, huh? Only about 4~5 μs slower than the baseline due to the dynamic construction of our computational graph in runtime and Flux is the fastest (it is implemented in similar approach), amazing! It is about 5x faster than other packages in either Julia or Python/C++.</p>
<p>So, as you see, writing an AD package can be super sweet in Julia with multiple dispatch. You can actually write your own AD with reasonable performance in Julia like a pro!</p>
<h2 id="Acknowledgement"><a href="#Acknowledgement" class="headerlink" title="Acknowledgement"></a>Acknowledgement</h2><p>Thanks for Keno for benchmarking advice on Zygote, I was actually quite confused about the performance and submitted an issue here: <a target="_blank" rel="noopener" href="https://github.com/FluxML/Zygote.jl/issues/28">Zygote.jl/issues/28</a></p>
<p>And thanks for the <a target="_blank" rel="noopener" href="https://github.com/JuliaGraphics/Luxor.jl">Luxor.jl</a> package, I use this for plotting the animation in this blog post. You might want to check my ugly plotting script here: <a target="_blank" rel="noopener" href="https://github.com/Roger-luo/YAAD.jl/blob/master/docs/plot.jl">plot.jl</a></p>
<p>Finally, thanks for Travis Ashworth for helping me on polishing the blog post. This is actually my first time to blog in English, and I didn’t check this blog post carefully. And now I have two Travis (another Travis is the Travis-CI which builds my blog automatically.)</p>
<!-- flag of hidden posts --></div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info"><a href="mailto:undefined">Jiaqi</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info"><a href="http://deal-ii.com/post/6bb18bdb/">http://deal-ii.com/post/6bb18bdb/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明来自 <a href="http://deal-ii.com" target="_blank">DEAL-II-Fluid</a>！</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/machine-learning/">machine learning</a><a class="post-meta__tags" href="/tags/parallel-computing/">parallel computing</a><a class="post-meta__tags" href="/tags/Julia/">Julia</a></div><div class="post_share"><div class="social-share" data-image="https://i.loli.net/2021/03/30/ULzlsMHcmNjKaQt.png" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css"/><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js"></script></div></div><nav class="pagination-post" id="pagination"></nav><div class="relatedPosts"><div class="relatedPosts_headline"><i class="fas fa-thumbs-up fa-fw"></i><span> 相关推荐</span></div><div class="relatedPosts_list"><div class="relatedPosts_item"><a href="/post/9864ea21/" title="machine learning for chaos system-执行篇"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/12/22/d7YjSwNqreOA91K.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-22</div><div class="relatedPosts_title">machine learning for chaos system-执行篇</div></div></a></div><div class="relatedPosts_item"><a href="/post/d03a598f/" title="machine learning for chaos system-测试篇"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/12/13/CRJOLzqKD6FlQWx.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-11</div><div class="relatedPosts_title">machine learning for chaos system-测试篇</div></div></a></div><div class="relatedPosts_item"><a href="/post/47358f5f/" title="machine learning for chaos system-应用"><img class="relatedPosts_cover" data-src="https://npg.copernicus.org/articles/27/373/2020/npg-27-373-2020-avatar-web.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-11</div><div class="relatedPosts_title">machine learning for chaos system-应用</div></div></a></div><div class="relatedPosts_item"><a href="/post/f2bbe0e7/" title="machine learning for chaos system-基础知识"><img class="relatedPosts_cover" data-src="https://i.loli.net/2020/12/13/KPc9zZ4hNn3CMta.png"><div class="relatedPosts_main is-center"><div class="relatedPosts_date"><i class="far fa-calendar-alt fa-fw"></i> 2020-12-11</div><div class="relatedPosts_title">machine learning for chaos system-基础知识</div></div></a></div></div></div><hr><div id="post-comment"><div class="comment-head"><div class="comment-headling"><i class="fas fa-comments fa-fw"></i><span> 评论</span></div></div><div class="comment-wrap"><div class="comments-items-1" data-name="Valine"><div class="vcomment" id="vcomment"></div><script>function loadvaline () {  
  var requestSetting = function (from,set) {
    var from = from
    var setting = set.split(',').filter(function(item){
    return from.indexOf(item) > -1
    });
    setting = setting.length == 0 ? from :setting;
    return setting
  }

  var guestInfo = requestSetting(['nick','mail','link'],'nick,mail,link')
  var requiredFields = requestSetting(['nick','mail'],'nick,mail')

  function initValine () {
    window.valine = new Valine({
      el:'#vcomment',
      appId: 'lJ3vEzEWaeVa6HpWNNbPrauw-gzGzoHsz',
      appKey: '16FI6zgDXwvw71g04yhHnnW0',
      placeholder: 'Please leave your footprints',
      avatar: 'monsterid',
      meta: guestInfo,
      pageSize: '10',
      lang: 'en',
      recordIP: false,
      serverURLs: '',
      emojiCDN: '',
      emojiMaps: "",
      enableQQ: false,
      requiredFields: requiredFields
    });
  }
  loadScript('https://cdn.jsdelivr.net/npm/valine/dist/Valine.min.js', initValine)
}

if ('Valine' === 'Valine' || false) {
  window.addEventListener('load', loadvaline)
}
else {
  function loadOtherComment () {
    loadvaline()
  }
}</script></div></div></div></article></main><footer id="footer" data-type="color"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2021  <i id="heartbeat" class="fa fas fa-heartbeat"></i> Jiaqi</div><div class="framework-info"><span>驱动 </span><a target="_blank" rel="noopener" href="https://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly"><span>Butterfly</span></a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><section id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="阅读模式"><i class="fas fa-book-open"></i></button><button id="font_plus" type="button" title="放大字体"><i class="fas fa-plus"></i></button><button id="font_minus" type="button" title="缩小字体"><i class="fas fa-minus"></i></button><button id="translateLink" type="button" title="简繁转换">繁</button><button id="darkmode" type="button" title="浅色和深色模式转换"><i class="fas fa-adjust"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="设置"><i class="fas fa-cog"></i></button><a id="to_comment" href="#post-comment" title="直达评论"><i class="fas fa-comments"></i></a><button class="close" id="mobile-toc-button" type="button" title="目录"><i class="fas fa-list-ul"></i></button><button id="go-up" type="button" title="回到顶部"><i class="fas fa-arrow-up"></i></button></div></section><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/tw_cn.js"></script><script src="https://cdn.jsdelivr.net/npm/medium-zoom/dist/medium-zoom.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  },
  CommonHTML: {
    linebreaks: { automatic: true, width: "90% container" }
  },
  "HTML-CSS": { 
    linebreaks: { automatic: true, width: "90% container" }
  },
  "SVG": { 
    linebreaks: { automatic: true, width: "90% container" }
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.jsdelivr.net/npm/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><script src="https://cdn.jsdelivr.net/npm/node-snackbar/dist/snackbar.min.js"></script><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><script src="https://cdn.jsdelivr.net/npm/instant.page/instantpage.min.js" type="module" defer></script><script src="https://cdn.jsdelivr.net/npm/vanilla-lazyload/dist/lazyload.iife.min.js" async></script><script src="/js/calendar.js"></script><script src="/js/languages.js"></script><script src="https://cdn.jsdelivr.net/npm/vue@2.6.11"></script><script src="/gitcalendar/js/gitcalendar.js"></script><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":120,"height":260},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>